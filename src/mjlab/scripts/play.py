"""Script to play RL agent with RSL-RL."""

import os
import sys
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Literal

import torch
import tyro
from rsl_rl.runners import OnPolicyRunner

from mjlab.envs import ManagerBasedRlEnv, ManagerBasedRlEnvCfg
from mjlab.rl import RslRlVecEnvWrapper
from mjlab.tasks.registry import list_tasks, load_env_cfg, load_rl_cfg
from mjlab.tasks.tracking.mdp import MotionCommandCfg
from mjlab.tasks.tracking.rl import MotionTrackingOnPolicyRunner
from mjlab.utils.os import get_wandb_checkpoint_path
from mjlab.utils.torch import configure_torch_backends
from mjlab.utils.wrappers import VideoRecorder
from mjlab.viewer import NativeMujocoViewer, ViserPlayViewer


@dataclass(frozen=True)
class PlayConfig:
  agent: Literal["zero", "random", "trained"] = "trained"
  registry_name: str | None = None
  wandb_run_path: str | None = None
  checkpoint_file: str | None = None
  motion_file: str | None = None
  num_envs: int | None = None
  device: str | None = None
  video: bool = False
  video_length: int = 200
  video_height: int | None = None
  video_width: int | None = None
  camera: int | str | None = None
  viewer: Literal["auto", "native", "viser"] = "auto"

  motion_command_sampling_mode: Literal["start", "uniform"] = "start"
  """Motion command sampling mode for tracking tasks."""


def _apply_play_env_overrides(
  cfg: ManagerBasedRlEnvCfg, motion_command_sampling_mode: Literal["start", "uniform"]
) -> None:
  """Apply PLAY mode overrides to an environment configuration.

  PLAY mode is used for inference/evaluation with trained agents. This function
  applies common overrides:
  - Sets infinite episode length.
  - Disables observation corruption.
  - Removes stochastic training events (e.g., push_robot).
  - Disables terrain curriculum if present.
  - Disables RSI randomization for tracking tasks.

  Args:
    cfg: The environment configuration to modify in-place.
  """
  # Infinite episodes for continuous inference.
  cfg.episode_length_s = int(1e9)

  # Disable observation corruption for clean state information.
  assert "policy" in cfg.observations
  cfg.observations["policy"].enable_corruption = False

  # Remove stochastic training events.
  assert cfg.events is not None
  cfg.events.pop("push_robot", None)

  # Disable terrain curriculum for rough terrain environments.
  if cfg.scene.terrain is not None:
    terrain_gen = cfg.scene.terrain.terrain_generator
    if terrain_gen is not None:
      terrain_gen.curriculum = False
      terrain_gen.num_cols = 5
      terrain_gen.num_rows = 5
      terrain_gen.border_width = 10.0

  # Disable RSI randomization for tracking tasks.
  if cfg.commands is not None and "motion" in cfg.commands:
    from mjlab.tasks.tracking.mdp import MotionCommandCfg

    motion_cmd = cfg.commands["motion"]
    assert isinstance(motion_cmd, MotionCommandCfg)
    motion_cmd.pose_range = {}
    motion_cmd.velocity_range = {}
    motion_cmd.sampling_mode = motion_command_sampling_mode


def run_play(task: str, cfg: PlayConfig):
  configure_torch_backends()

  device = cfg.device or ("cuda:0" if torch.cuda.is_available() else "cpu")

  env_cfg = load_env_cfg(task)
  _apply_play_env_overrides(env_cfg, cfg.motion_command_sampling_mode)

  agent_cfg = load_rl_cfg(task)

  DUMMY_MODE = cfg.agent in {"zero", "random"}
  TRAINED_MODE = not DUMMY_MODE

  # Check if this is a tracking task by checking for motion command.
  is_tracking_task = (
    env_cfg.commands is not None
    and "motion" in env_cfg.commands
    and isinstance(env_cfg.commands["motion"], MotionCommandCfg)
  )

  if is_tracking_task:
    assert env_cfg.commands is not None
    motion_cmd = env_cfg.commands["motion"]
    assert isinstance(motion_cmd, MotionCommandCfg)

    if DUMMY_MODE:
      if not cfg.registry_name:
        raise ValueError(
          "Tracking tasks require `registry_name` when using dummy agents."
        )
      # Check if the registry name includes alias, if not, append ":latest".
      registry_name = cfg.registry_name
      if ":" not in registry_name:
        registry_name = registry_name + ":latest"
      import wandb

      api = wandb.Api()
      artifact = api.artifact(registry_name)
      motion_cmd.motion_file = str(Path(artifact.download()) / "motion.npz")
    else:
      if cfg.motion_file is not None:
        print(f"[INFO]: Using motion file from CLI: {cfg.motion_file}")
        motion_cmd.motion_file = cfg.motion_file
      else:
        import wandb

        api = wandb.Api()
        if cfg.wandb_run_path is None and cfg.checkpoint_file is not None:
          raise ValueError(
            "Tracking tasks require `motion_file` when using `checkpoint_file`, "
            "or provide `wandb_run_path` so the motion artifact can be resolved."
          )
        if cfg.wandb_run_path is not None:
          wandb_run = api.run(str(cfg.wandb_run_path))
          art = next(
            (a for a in wandb_run.used_artifacts() if a.type == "motions"), None
          )
          if art is None:
            raise RuntimeError("No motion artifact found in the run.")
          motion_cmd.motion_file = str(Path(art.download()) / "motion.npz")

  log_dir: Path | None = None
  resume_path: Path | None = None
  if TRAINED_MODE:
    log_root_path = (Path("logs") / "rsl_rl" / agent_cfg.experiment_name).resolve()
    if cfg.checkpoint_file is not None:
      resume_path = Path(cfg.checkpoint_file)
      if not resume_path.exists():
        raise FileNotFoundError(f"Checkpoint file not found: {resume_path}")
      print(f"[INFO]: Loading checkpoint: {resume_path.name}")
    else:
      if cfg.wandb_run_path is None:
        raise ValueError(
          "`wandb_run_path` is required when `checkpoint_file` is not provided."
        )
      resume_path, was_cached = get_wandb_checkpoint_path(
        log_root_path, Path(cfg.wandb_run_path)
      )
      # Extract run_id and checkpoint name from path for display.
      run_id = resume_path.parent.name
      checkpoint_name = resume_path.name
      cached_str = "cached" if was_cached else "downloaded"
      print(
        f"[INFO]: Loading checkpoint: {checkpoint_name} (run: {run_id}, {cached_str})"
      )
    log_dir = resume_path.parent

  if cfg.num_envs is not None:
    env_cfg.scene.num_envs = cfg.num_envs
  if cfg.video_height is not None:
    env_cfg.viewer.height = cfg.video_height
  if cfg.video_width is not None:
    env_cfg.viewer.width = cfg.video_width

  render_mode = "rgb_array" if (TRAINED_MODE and cfg.video) else None
  if cfg.video and DUMMY_MODE:
    print(
      "[WARN] Video recording with dummy agents is disabled (no checkpoint/log_dir)."
    )
  env = ManagerBasedRlEnv(cfg=env_cfg, device=device, render_mode=render_mode)

  if TRAINED_MODE and cfg.video:
    print("[INFO] Recording videos during play")
    assert log_dir is not None  # log_dir is set in TRAINED_MODE block
    env = VideoRecorder(
      env,
      video_folder=log_dir / "videos" / "play",
      step_trigger=lambda step: step == 0,
      video_length=cfg.video_length,
      disable_logger=True,
    )

  env = RslRlVecEnvWrapper(env, clip_actions=agent_cfg.clip_actions)

  
  if DUMMY_MODE:
    action_shape: tuple[int, ...] = env.unwrapped.action_space.shape  # type: ignore
    if cfg.agent == "zero":

      class PolicyZero:
        def __call__(self, obs) -> torch.Tensor:
          del obs
          return torch.zeros(action_shape, device=env.unwrapped.device)

      policy = PolicyZero()
    else:

      class PolicyRandom:
        def __call__(self, obs) -> torch.Tensor:
          del obs
          return 2 * torch.rand(action_shape, device=env.unwrapped.device) - 1

      policy = PolicyRandom()
  else:
    if is_tracking_task:
      runner = MotionTrackingOnPolicyRunner(
        env, asdict(agent_cfg), log_dir=str(log_dir), device=device
      )
    else:
      runner = OnPolicyRunner(
        env, asdict(agent_cfg), log_dir=str(log_dir), device=device
      )
    runner.load(str(resume_path), map_location=device)
    policy = runner.get_inference_policy(device=device)

  #ADDED   
  import mujoco

  records = {"fl_foot_x": [], "fl_foot_y": [], "fl_foot_z": []}
  orig_policy = policy
  fl_site_id = None

  def _get_mj_model(base_env):
    for obj in (base_env, getattr(base_env, "sim", None), getattr(base_env, "_sim", None), getattr(base_env, "scene", None)):
      if obj is None:
        continue
      for attr in ("mj_model", "model", "m"):
        if hasattr(obj, attr):
          return getattr(obj, attr)
    return None

  def _get_mj_data(base_env):
    for obj in (base_env, getattr(base_env, "sim", None), getattr(base_env, "_sim", None), getattr(base_env, "scene", None)):
      if obj is None:
        continue
      for attr in ("mj_data", "data", "d"):
        if hasattr(obj, attr):
          return getattr(obj, attr)
    return None

  def recording_policy(obs):
    nonlocal fl_site_id

    with torch.no_grad():
      action = orig_policy(obs)

    base_env = env.unwrapped
    m = _get_mj_model(base_env)
    d = _get_mj_data(base_env)
    if m is None or d is None:
      raise RuntimeError("Couldn't access MuJoCo model/data to read FL site position.")

    if fl_site_id is None:
      fl_site_id = mujoco.mj_name2id(m, mujoco.mjtObj.mjOBJ_SITE, "robot/FL")
      if fl_site_id < 0:
        raise RuntimeError("MuJoCo model has no site named 'robot/FL'.")

    p = d.site_xpos[fl_site_id]  # (3,) world position
    records["fl_foot_x"].append(float(p[0]))
    records["fl_foot_y"].append(float(p[1]))
    records["fl_foot_z"].append(float(p[2]))

    return action

  policy = recording_policy



  # Handle "auto" viewer selection.
  if cfg.viewer == "auto":
    has_display = bool(os.environ.get("DISPLAY") or os.environ.get("WAYLAND_DISPLAY"))
    resolved_viewer = "native" if has_display else "viser"
    del has_display
  else:
    resolved_viewer = cfg.viewer

  if resolved_viewer == "native":
    NativeMujocoViewer(env, policy).run()
  elif resolved_viewer == "viser":
    ViserPlayViewer(env, policy).run()
  else:
    raise RuntimeError(f"Unsupported viewer backend: {resolved_viewer}")


  import numpy as np

  out_file = "left_foot_tracking_eval.npz"
  np.savez(out_file, **records)
  print(f"[INFO] Saved velocity tracking data to {out_file}")

  env.close()


def main():
  # Parse first argument to choose the task.
  # Import tasks to populate the registry.
  import mjlab.tasks  # noqa: F401

  all_tasks = list_tasks()
  chosen_task, remaining_args = tyro.cli(
    tyro.extras.literal_type_from_choices(all_tasks),
    add_help=False,
    return_unknown_args=True,
  )

  # Parse the rest of the arguments + allow overriding env_cfg and agent_cfg.
  agent_cfg = load_rl_cfg(chosen_task)

  args = tyro.cli(
    PlayConfig,
    args=remaining_args,
    default=PlayConfig(),
    prog=sys.argv[0] + f" {chosen_task}",
    config=(
      tyro.conf.AvoidSubcommands,
      tyro.conf.FlagConversionOff,
    ),
  )
  del remaining_args, agent_cfg

  run_play(chosen_task, args)


if __name__ == "__main__":
  main()
